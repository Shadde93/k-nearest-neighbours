{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NumPy, pandas and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter #added python standard library: collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(df,nobins=10,bintype='equal-width'):\n",
    "    cdf=df.copy()\n",
    "    numerical_cdf=df.copy()\n",
    "    for i in ['ID','CLASS']:\n",
    "        if i in cdf: numerical_cdf=numerical_cdf.drop([i], axis=1)\n",
    "    numerical_cdf=numerical_cdf.select_dtypes(include=['number'])\n",
    "    binfunc=[pd.cut,pd.qcut]\n",
    "    binning={}\n",
    "\n",
    "    if bintype==\"equal-width\": a=binfunc[0]\n",
    "    elif bintype==\"equal-size\": a=binfunc[1]\n",
    "    else: a=binfunc[0]\n",
    "\n",
    "    for i in numerical_cdf:\n",
    "        res,bins=a(cdf[i],nobins,retbins=True,labels=False,duplicates='drop')\n",
    "        bins[0]=-np.inf\n",
    "        bins[-1]=np.inf\n",
    "        binning[i]=bins\n",
    "        cdf[i]=res.astype('category')\n",
    "    return cdf,binning\n",
    "\n",
    "def apply_bins(df,binning):\n",
    "    cdf=df.copy()\n",
    "    numerical_cdf=df.copy()\n",
    "    for i in ['ID','CLASS']:\n",
    "        numerical_cdf=numerical_cdf.drop([i], axis=1)\n",
    "    numerical_cdf=numerical_cdf.select_dtypes(include=['number'])\n",
    "\n",
    "    for i in numerical_cdf:\n",
    "        bins=binning[i]\n",
    "        res,bins=pd.cut(cdf[i],bins,retbins=True,labels=False,duplicates='drop')\n",
    "        cdf[i]=res.astype('category')\n",
    "    return cdf\n",
    "\n",
    "\n",
    "def create_imputation(anneal_train_df):\n",
    "    anneal_train_df_copy = anneal_train_df.copy()\n",
    "    imputation_template = {}\n",
    "    \n",
    "    df_cols = anneal_train_df_copy.loc[:, ~anneal_train_df_copy.columns.isin(['ID', 'CLASS'])]\n",
    "    numerical_columns = df_cols.select_dtypes(include=['number']).columns\n",
    "    non_numerical_columns = df_cols.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "    anneal_train_df_numerical = anneal_train_df_copy.loc[:, numerical_columns]\n",
    "    for column in anneal_train_df_numerical.columns:\n",
    "        non_missing_col_data = anneal_train_df_numerical.loc[:,column]\n",
    "        if(non_missing_col_data.dropna().shape[0] > 0):\n",
    "            mean_value = non_missing_col_data.mean()\n",
    "        else:\n",
    "            mean_value = 0\n",
    "            \n",
    "        anneal_train_df_copy[column] = anneal_train_df_copy[column].fillna(mean_value)\n",
    "        imputation_template[column] = mean_value\n",
    "\n",
    "    anneal_train_df_categorical = anneal_train_df_copy.loc[:, non_numerical_columns]\n",
    "    for column in anneal_train_df_categorical.columns:\n",
    "        non_missing_col_data = anneal_train_df_categorical[column]\n",
    "        if(non_missing_col_data.dropna().shape[0] > 0):\n",
    "            mode_value = non_missing_col_data.mode()[0]\n",
    "        else:\n",
    "            mode_value = \"\"\n",
    "        \n",
    "        anneal_train_df_copy[column] = anneal_train_df_copy[column].fillna(mode_value)\n",
    "        imputation_template[column] = mode_value\n",
    "            \n",
    "        \n",
    "    return anneal_train_df_copy, imputation_template\n",
    "\n",
    "def apply_imputation(anneal_test_df, imputation):\n",
    "    anneal_test_df_copy = anneal_test_df.copy()\n",
    "    df_cols = anneal_test_df_copy.loc[:, ~anneal_test_df_copy.columns.isin(['ID', 'CLASS'])]\n",
    "    \n",
    "    for column in df_cols.columns:\n",
    "        anneal_test_df_copy[column] = anneal_test_df_copy[column].fillna(imputation[column])\n",
    "\n",
    "    return anneal_test_df_copy\n",
    "    \n",
    "\n",
    "\n",
    "def create_normalization(file_df,normalizationtype):\n",
    "    normalization = {}\n",
    "    new_file_df = file_df.copy()\n",
    "    \n",
    "    if (normalizationtype == \"minmax\"):\n",
    "        for col_name in file_df:\n",
    "            if (col_name != \"ID\" and col_name != \"CLASS\"):\n",
    "                min_col = file_df[col_name].min()\n",
    "                max_col = file_df[col_name].max()\n",
    "                normalization[col_name] = (\"minmax\", min_col,max_col)\n",
    "                new_file_df[col_name] = file_df[col_name].apply(lambda x:(x-min_col)/(max_col-min_col))\n",
    "        \n",
    "    elif (normalizationtype == \"zscore\"):\n",
    "        for col_name in file_df:\n",
    "            if (col_name != \"ID\" and col_name != \"CLASS\"):\n",
    "                mean_col = file_df[col_name].mean()\n",
    "                std_col = file_df[col_name].std()\n",
    "                normalization[col_name] = (\"zscore\", mean_col,std_col)\n",
    "                new_file_df[col_name] = file_df[col_name].apply(lambda x:(x-mean_col)/std_col)\n",
    "    \n",
    "    return (new_file_df, normalization)\n",
    "\n",
    "def apply_normalization(file_df,normalization):\n",
    "    \n",
    "    new_file_df = file_df.copy()\n",
    "    \n",
    "    for col_name in normalization:\n",
    "        \n",
    "        if (normalization[col_name][0] == \"minmax\"):\n",
    "            min_col = normalization[col_name][1]\n",
    "            max_col = normalization[col_name][2]\n",
    "            new_file_df[col_name] = file_df[col_name].apply(lambda x:(x-min_col)/(max_col-min_col))\n",
    "        \n",
    "        elif (normalization[col_name][0] == \"zscore\"):\n",
    "            mean_col = normalization[col_name][1]\n",
    "            std_col = normalization[col_name][2]\n",
    "            new_file_df[col_name] = file_df[col_name].apply(lambda x:(x-mean_col)/std_col)\n",
    "    \n",
    "    return (new_file_df)\n",
    "\n",
    "\n",
    "def create_one_hot(df_file):\n",
    "    one_hot_df = df_file.copy()\n",
    "    one_hot_dict = {}\n",
    "    for col_name in df_file:\n",
    "        if (col_name != \"CLASS\" and col_name != \"ID\"):\n",
    "            col = df_file[col_name]\n",
    "            col_hot = pd.get_dummies(col,prefix = col_name, dtype = \"float\")\n",
    "            one_hot_df = pd.concat([one_hot_df, col_hot], axis=1)\n",
    "            one_hot_df = one_hot_df.drop(col_name, axis=1)\n",
    "            one_hot_dict[col_name] = set(col)\n",
    "    \n",
    "    return (one_hot_df,one_hot_dict)\n",
    "\n",
    "\n",
    "def apply_one_hot(df_file, one_hot): \n",
    "    \n",
    "    #Gives zero column for categories who doesnt exist on the test df\n",
    "    \n",
    "    hot_df_file = df_file.copy()\n",
    "\n",
    "    for col_name, col_unique_values  in one_hot.items():\n",
    "        col = df_file[col_name]\n",
    "        cat = col.astype(pd.api.types.CategoricalDtype(categories= col_unique_values))\n",
    "        col_hot = pd.get_dummies(cat, prefix = col_name, dtype = \"float\")\n",
    "        hot_df_file = pd.concat([hot_df_file, col_hot], axis=1)\n",
    "        hot_df_file = hot_df_file.drop(col_name, axis=1)\n",
    "                                    \n",
    "    return (hot_df_file)\n",
    "\n",
    "def accuracy(pred_df, label):\n",
    "    \n",
    "    score = 0\n",
    "    for row in range(len(pred_df)):\n",
    "        pred_row = pred_df.iloc[row,:]\n",
    "        if pred_row.idxmax() == label[row]:\n",
    "            score = score + 1\n",
    "            \n",
    "    return (score/len(pred_df))\n",
    "\n",
    "\n",
    "def brier_score(df_file, correctlabels):\n",
    "    correctlabels_df = pd.DataFrame(correctlabels)\n",
    "    correctlabels_df = correctlabels_df.astype(pd.api.types.CategoricalDtype(categories= df_file.columns))\n",
    "    correctlabels_hot = pd.get_dummies(correctlabels_df)\n",
    "\n",
    "    score = []\n",
    "    for row in range(np.size(df_file,0)):\n",
    "        for col in range(np.size(df_file,1)):\n",
    "            score_each_element = df_file.iloc[row,col]- correctlabels_hot.iloc[row,col]\n",
    "            score_each_element = np.power(score_each_element, 2)\n",
    "            score.append(score_each_element)\n",
    "\n",
    "    brier_score = sum(score)/np.size(df_file,0)\n",
    "    \n",
    "    return(brier_score)\n",
    "\n",
    "\n",
    "def TP_AND_FP(predictions_decoded, correct_labels, positive_label):\n",
    "    \"\"\"Takes decoded predictions and labels\"\"\"\n",
    " \n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for prediction, label in zip(predictions_decoded.tolist(), correct_labels):\n",
    "        if prediction == 1:\n",
    "            if(label == positive_label):\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "           \n",
    "    num_positives = sum([1 for label in correct_labels if label == positive_label])\n",
    "    num_negatives = sum([1 for label in correct_labels if label != positive_label])\n",
    "   \n",
    "    return true_positives/num_positives, false_positives/num_negatives\n",
    "\n",
    "def auc_calc_np(tp_fp_dict):\n",
    "    unique_values = sorted( list(set([value for key, value in tp_fp_dict.items()])) )\n",
    "    tpr = np.array([value[0] for value in unique_values])\n",
    "    fpr = np.array([value[1] for value in unique_values])\n",
    " \n",
    "    area = np.trapz(tpr, fpr)\n",
    "    return area\n",
    "\n",
    "def binary_auc(predictions, correct_labels, positive_label='A', num_thresholds=10):\n",
    "    \"\"\"preprocesses the data into binary predictions, positives vs rest(negatives)\"\"\"\n",
    "    tp_fp_dict = {}\n",
    "    without_boundaries = np.linspace(0, 1, num_thresholds)\n",
    "    for threshold in without_boundaries:\n",
    "        predictions_copy = predictions.copy()\n",
    "        binary_positive_pred = predictions_copy.apply(lambda x: 1 if x[positive_label] >= threshold else 0, axis=1)\n",
    " \n",
    "        tp_fp_dict[threshold] = TP_AND_FP(binary_positive_pred, correct_labels, positive_label)\n",
    " \n",
    "    return auc_calc_np(tp_fp_dict)\n",
    " \n",
    "def auc(predictions, correct_labels, num_thresholds=100):\n",
    "    correct_labels = correct_labels.tolist() \n",
    "    label_frequencies = {x:correct_labels.count(x)/len(correct_labels) for x in correct_labels}\n",
    " \n",
    "    total_auc = 0\n",
    "    for label, label_frequency in label_frequencies.items():\n",
    "        auc = binary_auc(predictions, correct_labels, positive_label=label, num_thresholds=num_thresholds)\n",
    "        total_auc += auc*label_frequency\n",
    " \n",
    "    return total_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the class kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.imputation = None\n",
    "        self.normalization = None \n",
    "        self.one_hot = None \n",
    "        self.labels = None \n",
    "        self.training_labels = None \n",
    "        self.training_data = None \n",
    "        self.training_time = None \n",
    "        \n",
    "    def fit(self, dataframe, normtype = \"minmax\"): \n",
    "        \n",
    "        self.training_labels = pd.Series(dataframe[\"CLASS\"], dtype =\"category\")\n",
    "        self.labels = self.training_labels.cat.categories\n",
    "        cleaned_df = dataframe.drop([\"CLASS\" ,\"ID\"] , axis=1)  \n",
    "        self.imputation = create_imputation(cleaned_df)\n",
    "        self.normalization = create_normalization(self.imputation[0],normtype)\n",
    "        self.training_data = self.normalization[0].values\n",
    "\n",
    "    \n",
    "    def predict(self, df, k):\n",
    "\n",
    "        def get_nearest_neighbor_predictions(x_test, k = 5):\n",
    "            row_index_distance_list = []\n",
    "\n",
    "            for row_index,row_sample in enumerate(self.training_data):\n",
    "                dist = np.linalg.norm(row_sample-x_test)\n",
    "                row_index_distance_list.append((row_index,dist))\n",
    "  \n",
    "            row_index_distance_list.sort(key=lambda x:x[1])\n",
    " \n",
    "            row_index_list = [i[0] for i in row_index_distance_list]\n",
    "\n",
    "            k_near_index = row_index_list[0:k]\n",
    "\n",
    "            k_near_classes = self.training_labels[k_near_index]\n",
    "            \n",
    "            k_counter = Counter(k_near_classes)\n",
    "\n",
    "            len_k_near_classes = len(k_near_classes)\n",
    "            \n",
    "            test_class_prob = {y: k_counter.get(y, 0)/len_k_near_classes for y in self.labels}\n",
    "\n",
    "            return(test_class_prob)\n",
    "        \n",
    "        df_noClassID = df.drop([\"CLASS\" ,\"ID\"], axis=1)\n",
    "        \n",
    "        test_imp = apply_imputation(df_noClassID,self.imputation[1])\n",
    "        \n",
    "        test_norm = apply_normalization(test_imp,self.normalization[1])\n",
    "\n",
    "        test_data = test_norm.values\n",
    "        \n",
    "        df_prob = pd.DataFrame(columns=self.labels)\n",
    "        \n",
    "        for index ,row_test in enumerate(test_data):\n",
    "    \n",
    "            row_prob = get_nearest_neighbor_predictions(row_test , k)\n",
    "            \n",
    "            df_prob.loc[index] = row_prob\n",
    "        \n",
    "        return(df_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.06 s.\n",
      "Testing time (k=1): 0.78 s.\n",
      "Testing time (k=3): 0.91 s.\n",
      "Testing time (k=5): 0.72 s.\n",
      "Testing time (k=7): 0.42 s.\n",
      "Testing time (k=9): 0.49 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.663551</td>\n",
       "      <td>0.488058</td>\n",
       "      <td>0.813829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.474019</td>\n",
       "      <td>0.833424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.470723</td>\n",
       "      <td>0.834465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.483674</td>\n",
       "      <td>0.828734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Brier score       AUC\n",
       "1  0.747664     0.504673  0.758200\n",
       "3  0.663551     0.488058  0.813829\n",
       "5  0.579439     0.474019  0.833424\n",
       "7  0.598131     0.470723  0.834465\n",
       "9  0.616822     0.483674  0.828734"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "knn_model = kNN()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "knn_model.fit(glass_train_df)\n",
    "print(\"Training time: {0:.2f} s.\".format(time.perf_counter()-t0))\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "k_values = [1,3,5,7,9]\n",
    "results = np.empty((len(k_values),3))\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = knn_model.predict(glass_test_df,k=k_values[i])\n",
    "    print(\"Testing time (k={0}): {1:.2f} s.\".format(k_values[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=k_values,columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set (k=1): 1.00\n",
      "AUC on training set (k=1): 1.00\n",
      "Brier score on training set (k=1): 0.00\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "predictions = knn_model.predict(glass_train_df,k=1)\n",
    "print(\"Accuracy on training set (k=1): {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set (k=1): {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set (k=1): {0:.2f}\".format(brier_score(predictions,train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
